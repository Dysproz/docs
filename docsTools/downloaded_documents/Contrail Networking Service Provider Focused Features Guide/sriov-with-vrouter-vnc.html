<p id="topic-content"><h1 id="jd0e3">Configuring Single Root I/O Virtualization (SR-IOV)</h1><sw-topic-details date="2020-11-11"> </sw-topic-details><h2 id="jd0e13">Overview: Configuring SR-IOV</h2><p>Contrail Networking supports single root I/O virtualization
(SR-IOV) on Ubuntu systems and on Red Hat Enterprise Linux (RHEL)
operating systems as well.</p><p>SR-IOV is an interface extension of the PCI Express (PCIe) specification.
SR-IOV allows a device, such as a network adapter to have separate
access to its resources among various hardware functions.</p><p>As an example, the Data Plane Development Kit (DPDK) library
has drivers that run in user space for several network interface cards
(NICs). However, if the application runs inside a virtual machine
(VM), it does not see the physical NIC unless SR-IOV is enabled on
the NIC. </p><p>This topic shows how to configure SR-IOV with your Contrail
Networking system.</p><h2 id="jd0e24">Enabling ASPM in BIOS</h2><p>To use SR-IOV, it must have Active State Power Management (ASPM)
enabled for PCI Express (PCIe) devices. Enable ASPM in the system
BIOS.</p><sw-admonition name="note" style=""><strong class="title">Note</strong><p>The BIOS of your system might need to be upgraded to a version
that can enable ASPM.</p></sw-admonition><h2 id="jd0e32">Configuring SR-IOV Using the Ansible Deployer</h2><p>You must perform the following tasks to enable SR-IOV on a system.</p><ol type="1"><li id="jd0e39" style="">Enable the Intel Input/Ouput Memory Management Unit (IOMMU)
on Linux.</li><li id="jd0e42" style="">Enable the required number of Virtual Functions (VFs)
on the selected NIC.</li><li id="jd0e45" style="">Configure the names of the physical networks whose VMs
can interface with the VFs.</li><li id="jd0e48" style="">Reboot Nova compute.<div class="sample" dir="ltr" id="jd0e51"><div dir="ltr" id="jd0e52"><code>service nova-compute restart</code></div></div></li><li id="jd0e54" style="">Configure a Nova Scheduler filter based on the new PCI
configuration, as in the following example:<div class="sample" dir="ltr" id="jd0e57"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>/etc/nova/nova.conf
     [default]
     scheduler_default_filters = PciPassthroughFilter
     scheduler_available_filters = nova.scheduler.filters.all_filters
     scheduler_available_filters =
nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter
</pre></template></sw-code></div></div></li><li id="jd0e60" style="">Restart Nova Scheduler.<div class="sample" dir="ltr" id="jd0e63"><div dir="ltr" id="jd0e64"><code>service nova-scheduler restart</code></div></div></li></ol><p>The above tasks are handled by the Ansible Deployer playbook.
The cluster members and its configuration parameters are specified
in the <code class="filepath">instances.yaml</code> file located
in the config directory within the ansible-deployer repository.</p><p>The compute instances that are going to be in SR-IOV mode should
have an SR-IOV configuration. The <code class="filepath">instance.yaml</code> snippet below shows a sample instance definition.</p><div class="sample" dir="ltr" id="jd0e76"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>instances:
  bms1:
    provider: bms
    ip: <var v-pre="">ip-address</var>
    roles:
      openstack:
  bms2:
    provider: bms
    ip:<var v-pre="">ip-address</var>
    roles:
      config_database:
      config:
      control:
      analytics_database:
      analytics:
      webui:
  bms3:
    provider: bms
    ip: <var v-pre="">ip-address</var>
    roles:
      openstack_compute:
      vrouter:
        SRIOV: true
        SRIOV_VF: 3
        SRIOV_PHYSICAL_INTERFACE: eno1
        SRIOV_PHYS_NET:  physnet1
</pre></template></sw-code></div></div><h2 id="jd0e88">Configuring SR-IOV Using Helm</h2><p>You must perform the following tasks to enable SR-IOV on a system.</p><ol type="1"><li id="jd0e95" style="">Enable the Intel Input/Ouput Memory Management Unit (IOMMU)
on Linux.</li><li id="jd0e98" style="">Enable the required number of Virtual Functions (VFs)
on the selected NIC.</li><li id="jd0e101" style="">Configure the names of the physical networks whose VMs
can interface with the VFs.</li><li id="jd0e104" style="">Reboot Nova compute.<div class="sample" dir="ltr" id="jd0e107"><div dir="ltr" id="jd0e108"><code>service nova-compute restart</code></div></div></li><li id="jd0e110" style="">Configure a Nova Scheduler filter based on the new PCI
configuration, as in the following example:<div class="sample" dir="ltr" id="jd0e113"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>/etc/nova/nova.conf
      [default]
      scheduler_default_filters = PciPassthroughFilter
      scheduler_available_filters = nova.scheduler.filters.all_filters
      scheduler_available_filters =
      nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter
</pre></template></sw-code></div></div></li><li id="jd0e116" style="">Restart Nova Scheduler.<div class="sample" dir="ltr" id="jd0e119"><div dir="ltr" id="jd0e120"><code>service nova-scheduler restart</code></div></div></li></ol><p>The above tasks are handled by the Helm charts. The cluster
members and its configuration parameters are specified in the <code class="filepath">multinode-inventory</code> file located in the config directory
within the openstack-helm-infra repository.</p><p>For Helm, the configuration and SR-IOV environment-specific
parameters must be updated in three different places:</p><ul><li style=""><p>The compute instance must be set as contrail-vrouter-sriov.</p><p>For example, the following is a snippet from the <code class="filepath">tools/gate/devel/multinode-inventory.yaml</code> file in
the openstack-helm-infra repository.</p><div class="sample" dir="ltr" id="jd0e139"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>all:
  children:
    primary:
      hosts:
        node1:
          ansible_port: 22
          ansible_host: <var v-pre="">host-ip-address</var>
          ansible_user: ubuntu
          ansible_ssh_private_key_file: /home/ubuntu/.ssh/insecure.pem
          ansible_ssh_extra_args: -o StrictHostKeyChecking=no
  nodes:
    children:
      openstack-compute:
        children:
           contrail-vrouter-sriov: #compute instance set to contrail-vrouter-sriov
              hosts:
                node7:
                ansible_port: 22
                 ansible_host: <var v-pre="">host-ip-address</var>
                 ansible_user: ubuntu
                 ansible_ssh_private_key_file: /home/ubuntu/.ssh/insecure.pem
                 ansible_ssh_extra_args: -o StrictHostKeyChecking=no
</pre></template></sw-code></div></div></li><li style=""><p>Contrail-vrouter-sriov must be labeled appropriately.</p><p>For example, the following is a snippet from the<code class="filepath"> tools/gate/devel/multinode-vars.yaml</code> in the openstack-helm-infra
repository.</p><div class="sample" dir="ltr" id="jd0e156"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>nodes:
  labels:
    primary:
    - name: openstack-helm-node-class
      value: primary

            all:
    	 - name: openstack-helm-node-class
      	   value: general
    	 contrail-controller:
    	 - name: opencontrail.org/controller
               value: enabled
    	 openstack-compute:
    	 - name: openstack-compute-node
               value: enabled
            contrail-vrouter-dpdk:
            - name: opencontrail.org/vrouter-dpdk
                  value: enabled
             contrail-vrouter-sriov: # label as contrail-vrouter-sriov
             - name: vrouter-sriov
                 value: enabled
</pre></template></sw-code></div></div></li><li style=""><p>SR-IOV config parameters must be updated in the <code class="filepath">contrail-vrouter/values.yaml</code> file. </p><p>For example, the following is a snippet from the <code class="filepath">contrail-vrouter/values.yaml</code> file in the contrail-helm-deployer
repository.</p><div class="sample" dir="ltr" id="jd0e170"><div class="output" dir="ltr"><sw-code><template v-pre=""><pre>contrail_env_vrouter_kernel:
  AGENT_MODE: kernel

contrail_env_vrouter_sriov:
  SRIOV: true
  per_compute_info:
    node_name: k8snode1
    SRIOV_VF:  10
    SRIOV_PHYSICAL_INTERFACE: enp129s0f1
    SRIOV_PHYS_NET:  physnet1
</pre></template></sw-code></div></div></li></ul><h2 id="jd0e173">Launching SR-IOV Virtual Machines </h2><div class="mini-toc-intro"><p>After ensuring that SR-IOV features are enabled on your system,
use one of the following procedures to create a virtual network from
which to launch an SR-IOV VM, either by using the Contrail Web UI
or the CLI. Both methods are included.</p></div><ul><li style=""><p><a href="sriov-with-vrouter-vnc.html#jd0e181">Using the Contrail Web UI to Enable and Launch an SR-IOV Virtual
Machine</a></p></li><li style=""><p><a href="sriov-with-vrouter-vnc.html#jd0e248">Using the CLI to Enable and Launch SR-IOV Virtual Machines</a></p></li></ul><h3 id="jd0e181">Using the Contrail Web UI to Enable and Launch an SR-IOV Virtual
Machine</h3><p>To use the Contrail Web UI to enable and launch an SR-IOV VM:</p><ol type="1"><li id="jd0e188" style="">At <strong v-pre="">Configure &gt; Networking &gt; Networks</strong>, create
a virtual network with SR-IOV enabled. Ensure the virtual network
is created with a subnet attached. In the Advanced section, select
the<strong v-pre=""> Provider Network</strong> check box, and specify the physical
network already enabled for SR-IOV (in <code class="inline" v-pre="">testbed.py</code> or <code class="inline" v-pre="">nova.conf</code>) and its VLAN ID. See <a href="sriov-with-vrouter-vnc.html#sriov1">Figure 1</a>.<figure id="sriov1"><figcaption>Figure 1: Edit Network</figcaption><div class="graphic"><img alt="Edit Network" src="documentation/images/S018550.png" style=""/></div></figure></li><li id="jd0e209" style="">On the virtual network, create a Neutron port (<strong v-pre="">Configure
&gt; Networking &gt; Ports</strong>), and in the <strong v-pre="">Port Binding </strong>section,
define a <strong v-pre="">Key</strong> value of SR-IOV and a <strong v-pre="">Value</strong> of
direct. See <a href="sriov-with-vrouter-vnc.html#sriov2">Figure 2</a>.<figure id="sriov2"><figcaption>Figure 2: Create Port</figcaption><div class="graphic"><img alt="Create Port" src="documentation/images/S018551.png" style=""/></div></figure></li><li id="jd0e230" style="">Using the UUID of the Neutron port you created, use the <code class="inline" v-pre="">nova boot</code> command to launch the VM from that port.<p><code class="inline" v-pre="">        nova boot --flavor m1.large --image <var v-pre="">&lt;image name&gt;</var> --nic port-id=<var v-pre="">&lt;uuid of above
port&gt;</var> <var v-pre="">&lt;vm name&gt;</var> </code></p></li></ol><h3 id="jd0e248">Using the CLI to Enable and Launch SR-IOV Virtual Machines</h3><p>To use CLI to enable and launch an SR-IOV VM:</p><ol type="1"><li id="jd0e255" style="">Create a virtual network with SR-IOV enabled. Specify
the physical network already enabled for SR-IOV (in <code class="inline" v-pre="">testbed.py</code> or <code class="inline" v-pre="">nova.conf</code>) and its VLAN ID. <p>The following example creates <code class="inline" v-pre="">vn1</code> with a VLAN ID of 100 and is part of <code class="inline" v-pre="">physnet1</code>:</p><p><code class="inline" v-pre="">        neutron net-create  --provider:physical_network=physnet1
 --provider:segmentation_id=100 vn1</code></p></li><li id="jd0e275" style="">Create a subnet in vn1.<p><code class="inline" v-pre="">        neutron subnet-create vn1 a.b.c.0/24</code></p></li><li id="jd0e281" style="">On the virtual network, create a Neutron port on the subnet,
with a binding type of direct.<p><code class="inline" v-pre="">        neutron port-create --fixed-ip subnet_id=&lt;subnet
uuid&gt;,ip_address=&lt;IP address from above subnet&gt; --name &lt;name
of port&gt; &lt;vn uuid&gt;  --binding:vnic_type direct</code> </p></li><li id="jd0e289" style="">Using the UUID of the Neutron port created, use the <code class="inline" v-pre="">nova boot</code> command to launch the VM from that port.<p> <code class="inline" v-pre="">nova boot --flavor m1.large --image <var v-pre="">&lt;image name&gt;</var> --nic port-id=<var v-pre="">&lt;uuid of above
port&gt;</var> <var v-pre="">&lt;vm name&gt;</var> </code></p></li><li id="jd0e308" style="">Log in to the VM and verify that the Ethernet controller
is VF by using the <code class="inline" v-pre="">lspci</code> command to list
the PCI buses. <p>The VF that gets configured with the VLAN can be observed using
the <code class="inline" v-pre="">ip link</code> command.</p></li></ol><sw-prev-next> </sw-prev-next></p>